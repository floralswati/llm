import transformers
from transformers import pipeline, AutoTokenizer
from docx import Document

model_cache = {}
tokenizer_cache = {}

# Function to create a summarization pipeline for a given model
def create_summarizer(model_name):
    if model_name not in model_cache:
        try:
            summarizer = pipeline('summarization',
                                  model=model_name,
                                  max_new_tokens=100,  # Limit the number of new tokens generated
                                  num_beams=4,
                                  early_stopping=True)
            model_cache[model_name] = summarizer
        except Exception as e:
            print(f"Error loading model '{model_name}': {e}")
            return None
    return model_cache.get(model_name)

# Function to get the tokenizer for a given model
def get_tokenizer(model_name):
    if model_name not in tokenizer_cache:
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            tokenizer_cache[model_name] = tokenizer
        except Exception as e:
            print(f"Error loading tokenizer '{model_name}': {e}")
            return None
    return tokenizer_cache.get(model_name)

# Function to extract text from a docx file
def extract_text_fromdocx(filename):
    document = Document(filename)
    return [p.text for p in document.paragraphs]

# Function to chunk text into smaller pieces based on tokens
def chunk_text(text, tokenizer, max_length=512):
    tokens = tokenizer(text, truncation=False, return_tensors='pt')['input_ids'][0]
    for i in range(0, len(tokens), max_length):
        yield tokenizer.decode(tokens[i:i + max_length], skip_special_tokens=True)

# Function to generate summaries using a given list of summarizers
def generate_summaries(paragraph_texts, summarizers):
    summaries = {}
    for model_name, summarizer in summarizers.items():
        if summarizer is not None:
            try:
                combined_text = " ".join(paragraph_texts)  # Combine all paragraphs into a single text
                tokenizer = get_tokenizer(model_name)
                if tokenizer is None:
                    continue

                chunks = list(chunk_text(combined_text, tokenizer))
                chunk_summaries = []

                for chunk in chunks:
                    summary = summarizer(chunk, max_length=100, num_beams=4, early_stopping=True)
                    if isinstance(summary, list):
                        chunk_summaries.append(summary[0]['summary_text'])
                    else:
                        chunk_summaries.append(summary['summary_text'])

                combined_summary = " ".join(chunk_summaries)

                # Truncate summary to 100 words if necessary
                summary_words = combined_summary.split()
                if len(summary_words) > 100:
                    combined_summary = " ".join(summary_words[:100])

                summaries[model_name] = combined_summary
            except Exception as e:
                print(f"Error generating summary for '{model_name}': {e}")
        else:
            print(f"Skipping '{model_name}' (model not loaded)")
    return summaries

def main(filename):
    paragraph_texts = extract_text_fromdocx(filename)
    model_names = [
        "facebook/bart-large-cnn",
        "google/flan-t5-base",
        "google-t5/t5-base",
        # "gmicrosoft/Phi-3-mini-128k-instruct",
        # "apple/OpenELM-3B-Instruct"
    ]
    summarizers = {model_name: create_summarizer(model_name) for model_name in model_names}
    summaries = generate_summaries(paragraph_texts, summarizers)
    for model_name, summary in summaries.items():
        word_count = len(summary.split())
        print(f"Summary generated by {model_name} (Word count: {word_count}):\n\n")
        print(f"{summary}\n")

if __name__ == "__main__":
    main('The Boy Who Cried Wolf.docx')
