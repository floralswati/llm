import transformers
import torch
from transformers import pipeline
from transformers import GPT2Tokenizer
from transformers import GPT2Model
from huggingface_hub import login
from langchain import LLMChain, PromptTemplate


model_cache = {}

# Function to create a summarization pipeline for a given model
def create_summarizer(model_name):
    if model_name not in model_cache:
        try:
           summarizer = pipeline("text2text-generation", model=model_name, max_length=50, num_beams=4, early_stopping=True)
           model_cache[model_name] = summarizer
        except Exception as e:
            print(f"Error loading model '{model_name}': {e}")
            return None
    return model_cache.get(model_name)


def generate_summaries(document, summarizers):
    summaries = {}
    for model_name, summarizer in summarizers.items():
        if summarizer is not None:
            try:
                summary = summarizer(document, max_length=350, num_beams=4, early_stopping=True)
                if isinstance(summary, list):
                    summaries[model_name] = summary[0]['generated_text']
                else:
                    summaries[model_name] = summary['generated_text']
            except Exception as e:
                print(f"Error generating summary for '{model_name}': {e}")
        else:
            print(f"Skipping '{model_name}' (model not loaded)")
    return summaries

# Main function
def main(document):
    model_names = [
        #"meta-llama/Llama-2-7b-chat-hf",
        "google-t5/t5-base",
        #"google-bert/bert-base-uncased",
        #"mistralai/Mixtral-8x7B-Instruct-v0.1",
        #"facebook/bart-large-cnn",
        #"openai-community/gpt2-medium",
       # "pegasus-cnn-568M"
    ]

    summarizers = {model_name: create_summarizer(model_name) for model_name in model_names}
    summaries = generate_summaries(document, summarizers)

    for model_name, summary in summaries.items():
        print(f"Summary generated by {model_name}:\n{summary}\n")

# Example usage
document = "This is a sample document with around 200-250 words for testing abstractive text summarization using various open-source language models like Meta-LLaMA, Google T5, Google BERT, Mistral AI, and Facebook BART. The goal is to generate concise summaries that capture the essence of the document while maintaining coherence and informativeness. The summarization process involves understanding the context, identifying key information, and presenting it in a condensed form. The quality of the summaries can be evaluated based on metrics such as ROUGE scores, factual accuracy, and readability. This example demonstrates how to leverage different language models for the task of abstractive summarization, enabling comparison and analysis of their respective strengths and limitations."

if __name__ == "__main__":
    main(document)
